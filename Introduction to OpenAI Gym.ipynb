{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started With OpenAI Gym: The Basic Building Blocks\n",
    "\n",
    "This tutorial will cover all of the basic building blocks of OpenAI Gym, including environments, spaces, wrappers, and vectorized environments. You can follow along with the full tutorial [on the Paperspace blog](https://blog.paperspace.com/getting-started-with-openai-gym/). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation \n",
    "\n",
    "First thing's first, we begin with the installation of the OpenAI gym. We can install gym from pip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environments \n",
    "\n",
    "The fundamental building block of OpenAI gym is the `Env` class. This is a Python class that basically implements a simulator that runs the  environment you want to train your agent in. OpenAI Gym comes packed with a lot of environments, such as ones where you move a car up a hill, balance a swinging pendulum, atari games, etc. Gym also provides you with the ability to create your custom environments. \n",
    "\n",
    "We start with an environment called `MountainCar` where the objective is to drive a car up a mountain. The car is on a one-dimensional track positioned between two \"mountains\". The goal is to drive up the mountain on the right; however, the car's engine is not strong enough to scale the mountain in a single pass. Therefore, the only way to succeed is to drive back and forth to build up momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyvirtualdisplay.display.Display at 0x7f5cfc345cc0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0,size=(600,600))\n",
    "display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('MountainCar-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic structure of the environment is described by the `observation_space` and the `action_space` attributes of the Gym `Env` class. \n",
    "\n",
    "`observation_space` defines the structure as well as the legitimate values for the observation of the state of the environment. The observation can be different things for each environment. The most common form is a screenshot of the game. There can be other forms of observations as well, such as certain characteristics of the environment described in vector form. \n",
    "\n",
    "Similarly, the `Env` class also defines an attribute called the `action_space`, which describes the numerical structure of the legitimate actions that can be applied to the environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The observation space: Box(2,)\n",
      "The action space: Discrete(3)\n"
     ]
    }
   ],
   "source": [
    "# Observation and action space \n",
    "obs_space = env.observation_space\n",
    "action_space = env.action_space\n",
    "print(\"The observation space: {}\".format(obs_space))\n",
    "print(\"The action space: {}\".format(action_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observation for the mountain car is a vector of two numbers representing velocity and position. The middle point between the two mountains is taken to be the origin, with right being the positive direction and left being the negative. \n",
    "\n",
    "We see that both the observation space as well as the action space are represented by classes called `Box` and `Discrete`. These are one of the various data structures provided by `Gym` in order to implement observation and action spaces for different kinds of scenarios (discrete action space, continous action space, etc.). We will dig further into these later in the tutorial. \n",
    "\n",
    "## Interacting With the Environment \n",
    "\n",
    "In this section, we cover functions of the `Env` class that help the agent interact with the environment. Such functions are:\n",
    "\n",
    "1. `reset`: This function resets the environment to its initial state and returns the observation of the environment corresponding to the initial state. \n",
    "2. `step` : This function takes an action as an input and applies it to the the environment, which leads to the environment transitioning to a new state. The reset function returns you 4 things:\n",
    "    * `observation`: The observation of the state of the environment.\n",
    "    * `reward`: The reward that you get from the environment after executing the action that was given as the input to the `step` function. \n",
    "    * `done`: Whether the episode has been terminated. If true, you may need to end the simulation or reset the environment to restart the episode. \n",
    "    * `info`: This provides additional information depending upon the environment, such as number of lives left, or general information that may be conducive in debugging.\n",
    "\n",
    "Let us now see an example that illustrates the concepts discussed above. We first begin by resetting the environment, then we inspect an observation. We then apply an action and inspect the new observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The initial observation is [-0.54013665  0.        ]\n",
      "The new observation is [-5.40012672e-01  1.23983216e-04]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# reset the environment and see the initial observation\n",
    "obs = env.reset()\n",
    "print(\"The initial observation is {}\".format(obs))\n",
    "\n",
    "# Sample a random action from the entire action space\n",
    "random_action = env.action_space.sample()\n",
    "\n",
    "# # Take the action and get the new observation space\n",
    "new_obs, reward, done, info = env.step(random_action)\n",
    "print(\"The new observation is {}\".format(new_obs))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, our observation is not the screenshot of the task being performed. In many other environments (like Atari, as we will see) the observation is a screenshot of the game. In either of the scenarios, if you want to see what the environment looks like in the current state, you can use the `render` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.render(mode = \"human\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should display the environment in its current state in a pop-up window. You can close the window using the `close` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would rather like the screenshot of the game as an image, rather than a pop-up window, you should set the `mode` argument of the `render` function to `rgb_array`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f5cb86ca9b0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env_screen = env.render(mode = 'rgb_array')\n",
    "env.close()\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "plt.imshow(env_screen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collecting all the tiny little blocks we have covered so far, the typical code for running your agent inside the `MountainCar` environment would look like the following. In our case, we just take random actions, but you can have an agent that does something more intelligent based on the observation you get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "# Number of steps you run the agent for \n",
    "num_steps = 1500\n",
    "\n",
    "obs = env.reset()\n",
    "\n",
    "for step in range(num_steps):\n",
    "    # take random action, but you can also do something more intelligent\n",
    "    # action = my_intelligent_agent_fn(obs) \n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    # apply the action\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    \n",
    "    # Render the env\n",
    "    env.render()\n",
    "\n",
    "    # Wait a bit before the next frame unless you want to see a crazy fast video\n",
    "    time.sleep(0.001)\n",
    "    \n",
    "    # If the epsiode is up, then start another one\n",
    "    if done:\n",
    "        env.reset()\n",
    "\n",
    "# Close the env\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spaces\n",
    "\n",
    "The `observation_space` for our environment was `Box(2,)` and the `action_space` was `Discrete(2,)`. What do these actually mean? Both `Box` and `Discrete` are types of a data structures called `Spaces` provided by Gym to describe the legitimate values for the observation and actions for the environments. \n",
    "\n",
    "All of these data structures are derived from the `gym.Space` base class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gym.spaces.box.Box"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Box(n,)` corresponds to the n-dimensional continuous space `Discrete(2,)`. In our case `n=2`, and thus the observational space of our environment is a 2-D space. Of course, the space is bounded by upper and lower limits which describe the legitimate values our observations can take. We can determine this using `high` and `low` attributes of the observation space. These correspond to the maximum and minimum positions/velocities in our environment, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upper Bound for Env Observation [0.6  0.07]\n",
      "Lower Bound for Env Observation [-1.2  -0.07]\n"
     ]
    }
   ],
   "source": [
    "print(\"Upper Bound for Env Observation\", env.observation_space.high)\n",
    "print(\"Lower Bound for Env Observation\", env.observation_space.low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can set these upper/lower limits while defining your space as well as when you are creating an environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Discrete(n)` box describes a discrete space with `[0..n-1]` possible values. In our case `n = 3`, meaning our actions can take values of either 0, 1, or 2. Unlike `Box`, `Discrete` does not have a `high` and `low` method, since, by the very definition, it is clear what values are allowed. \n",
    "\n",
    "If you try to input invalid values (in our case, say, 4) in the `step` function of our environment, it will lead to an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It works!\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "4 (<class 'int'>) invalid",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-d0288479e6c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Doesn't work.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"It works!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/envs/classic_control/mountain_car.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"%r (%s) invalid\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mposition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvelocity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: 4 (<class 'int'>) invalid"
     ]
    }
   ],
   "source": [
    "# Works \n",
    "env.step(2)\n",
    "print(\"It works!\")\n",
    "\n",
    "# Doesn't work.\n",
    "env.step(4)\n",
    "print(\"It works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple other spaces available for various use cases, such as `MultiDiscrete` which allows you to use  more than one discrete variable for your observation and action space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrappers \n",
    "\n",
    "\n",
    "The `Wrapper` class in OpenAI Gym provides you with the functionality to modify various parts of an environment to suit your needs. Why might such a need arise? Maybe you want to normalize your pixel input, or maybe you want to clip your rewards. While typically you could accomplish the same by making another class that subclasses your environment `Env` class, `Wrapper` class allows us to do it more systematically. \n",
    "\n",
    "But before we begin, let's switch to a more complex environment that will really help us appreciate the utility that `Wrapper` brings to the table. This complex environment is going to be the the Atari game Breakout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Space:  Box(210, 160, 3)\n",
      "Action Space        Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"BreakoutNoFrameskip-v4\")\n",
    "\n",
    "print(\"Observation Space: \", env.observation_space)\n",
    "print(\"Action Space       \", env.action_space)\n",
    "\n",
    "\n",
    "obs = env.reset()\n",
    "\n",
    "for i in range(1000):\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    time.sleep(0.01)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our observation space is a continous space of dimensions (210, 160, 3), corresponding to a RGB pixel observation of the same size, and our action space contains contains 4 discrete actions (Left, Right, Do Nothing, and Fire).\n",
    "\n",
    "Now that we have our environment loaded, let us suppose we have to make certain changes to the Atari Environment. It's common practice in Deep RL that we construct our observation by concatenating the past `k` frames together. We have to modify the Breakout Environment such that both our `reset` and `step` function return concatenated observations. \n",
    "\n",
    "For this we define a class of type `gym.Wrappers`to override the `reset` and `return` functions of the Breakout Env. The `Wrapper` class, as the name suggests, is a wrapper on top of an `Env` class that modifies some of its attributes and functions.\n",
    "\n",
    "The `__init__` function is defined with the `Env` class for which the wrapper is written and a number of past frames to be concatenated. Note that we also need to redefine the observation space since we are now using concatenated frames. (We modify the observation space from (210, 160, 3) to (210, 160, 3 * num_past_frames))\n",
    "\n",
    "In the `reset` function, while we are initializing the environment, since we don't have any previous observations to concatenate, we concatenate the initial observations repeatedly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "\n",
    "class ConcatObs(gym.Wrapper):\n",
    "    def __init__(self, env, k):\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.k = k\n",
    "        self.frames = deque([], maxlen=k)\n",
    "        shp = env.observation_space.shape\n",
    "        self.observation_space = \\\n",
    "            spaces.Box(low=0, high=255, shape=((k,) + shp), dtype=env.observation_space.dtype)\n",
    "\n",
    "\n",
    "def reset(self):\n",
    "    ob = self.env.reset()\n",
    "    for _ in range(self.k):\n",
    "        self.frames.append(ob)\n",
    "    return self._get_ob()\n",
    "\n",
    "def step(self, action):\n",
    "    ob, reward, done, info = self.env.step(action)\n",
    "    self.frames.append(ob)\n",
    "    return self._get_ob(), reward, done, info\n",
    "\n",
    "def _get_ob(self):\n",
    "    return np.array(self.frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to effectively get our modified environment, we wrap our environment `env` in the wrapper we just created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new observation space is Box(4, 210, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"BreakoutNoFrameskip-v4\")\n",
    "wrapped_env = ConcatObs(env, 4)\n",
    "print(\"The new observation space is\", wrapped_env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now verify whether the observations are indeed concatenated or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intial obs is of the shape (210, 160, 3)\n",
      "Obs after taking a step is (210, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "# Reset the Env\n",
    "obs = wrapped_env.reset()\n",
    "print(\"Intial obs is of the shape\", obs.shape)\n",
    "\n",
    "# Take one step\n",
    "obs, _, _, _  = wrapped_env.step(2)\n",
    "print(\"Obs after taking a step is\", obs.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is more to gym's wrappers than the vanilla `Wrapper` class. Gym also provides you with specific wrappers that target specific elements of the environment such as observations, rewards, and actions. Their use is demonstrated in the following section. \n",
    "\n",
    "1. `ObservationWrapper`: This helps us make changes to the observation using the `observation` method of the wrapper class.\n",
    "2. `RewardWrapper`: This helps us make changes to the reward using the `reward` function of the wrapper class. \n",
    "3. `ActionWrapper`: This helps us make changes to the action using the `action` function of the wrapper class.\n",
    "\n",
    "Let us suppose we have to make the following changes to our environment:\n",
    "\n",
    "1. We have to normalize the pixel observations by 255. \n",
    "2. We have to  clip the rewards between 0 and 1. \n",
    "3. We have to prevent the slider from moving to the left (action 3).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "class ObservationWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "    \n",
    "    def observation(self, obs):\n",
    "        # Normalise observation by 255\n",
    "        return obs / 255.0\n",
    "\n",
    "class RewardWrapper(gym.RewardWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "    \n",
    "    def reward(self, reward):\n",
    "        # Clip reward between 0 to 1\n",
    "        return np.clip(reward, 0, 1)\n",
    "    \n",
    "class ActionWrapper(gym.ActionWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "    \n",
    "    def action(self, action):\n",
    "        if action == 3:\n",
    "            return random.choice([0,1,2])\n",
    "        else:\n",
    "            return action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we apply all these wrappers to our environment in a single line of code to get a modified environment. Then, we verify all our intended changes have been applied to the environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All checks passed\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"BreakoutNoFrameskip-v4\")\n",
    "wrapped_env = ObservationWrapper(RewardWrapper(ActionWrapper(env)))\n",
    "\n",
    "obs = wrapped_env.reset()\n",
    "\n",
    "for step in range(500):\n",
    "    action = wrapped_env.action_space.sample()\n",
    "    obs, reward, done, info = wrapped_env.step(action)\n",
    "    \n",
    "    # Raise a flag if values have not been vectorised properly\n",
    "    if (obs > 1.0).any() or (obs < 0.0).any():\n",
    "        print(\"Max and min value of observations out of range\")\n",
    "    \n",
    "    # Raise a flag if reward has not been clipped.\n",
    "    if reward < 0.0 or reward > 1.0:\n",
    "        assert False, \"Reward out of bounds\"\n",
    "    \n",
    "    # Check the rendering if the slider moves to the left.\n",
    "    wrapped_env.render()\n",
    "    \n",
    "    time.sleep(0.001)\n",
    "\n",
    "wrapped_env.close()\n",
    "\n",
    "print(\"All checks passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you want to recover the original `Env` after applying wrappers to it, you can use the `unwrapped` attribute of an `Env` class. While the `Wrapper` class may look like just any other class that subclasses from `Env`, it does maintain a list of wrappers applied to the base `Env`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapped Env: <ObservationWrapper<RewardWrapper<ActionWrapper<TimeLimit<AtariEnv<BreakoutNoFrameskip-v4>>>>>>\n",
      "Unwrapped Env <AtariEnv<BreakoutNoFrameskip-v4>>\n",
      "Getting the meaning of actions ['NOOP', 'FIRE', 'RIGHT', 'LEFT']\n"
     ]
    }
   ],
   "source": [
    "print(\"Wrapped Env:\", wrapped_env)\n",
    "print(\"Unwrapped Env\", wrapped_env.unwrapped)\n",
    "print(\"Getting the meaning of actions\", wrapped_env.unwrapped.get_action_meanings())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorized Environments\n",
    "\n",
    "A lot of deep RL algorithms like Asynchronous Actor Critic Methods use parallel threads, where each thread runs an instance of the environment to both speed up the training process and improve efficiency. \n",
    "\n",
    "For this we use another library, also by OpenAI, called `baselines`. This library provides us with performant implementations of many standard Deep RL algorithms to compare any novel algorithm to. In addition to these implementations, `baselines` also provides us with many other features that enable us to prepare our environments in accordance with the way they were used in OpenAI experiments. \n",
    "\n",
    "One of these features includes wrappers which allow you to run multiple environments in parallel using a single function call. Before we begin, we first proceed with the installation of baselines by running the following commands in a terminal. \n",
    "\n",
    "```\n",
    "!git clone https://github.com/openai/baselines\n",
    "!cd baselines\n",
    "!pip install . \n",
    "```\n",
    "\n",
    "You may need to restart your jupyter notebook for the installed package to be available. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The wrapper of interest here is called `SubProcEnv`. It will run all the environments in an asynchronous manner. We first create a list of function calls that return the environment we are running. In code, I have used a `lambda` function to create an anonymous function that returns the gym environment. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the neccasary stuff \n",
    "import gym\n",
    "from baselines.common.vec_env.subproc_vec_env import SubprocVecEnv\n",
    "\n",
    "# list of envs \n",
    "num_envs = 3\n",
    "envs = [lambda: gym.make(\"BreakoutNoFrameskip-v4\") for i in range(num_envs)]\n",
    "\n",
    "# Vec Env \n",
    "envs = SubprocVecEnv(envs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `envs` now acts as a single environment where we can call the `reset` and `step` functions. However, these functions return an array of observations/actions, rather than a single observation/actions now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Envs: 3\n",
      "Shape of one Env: (210, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "# Get initial state\n",
    "init_obs = envs.reset()\n",
    "\n",
    "\n",
    "# We get a list of observations corresponding to parallel environments \n",
    "print(\"Number of Envs:\", len(init_obs))\n",
    "\n",
    "# Check out of the obs \n",
    "one_obs = init_obs[0]\n",
    "print(\"Shape of one Env:\", one_obs.shape)\n",
    "\n",
    "# Prepare a list of actions and apply them to environment \n",
    "actions = [0, 1, 2]\n",
    "obs = envs.step(actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the `render` function on the vectorized `envs` displays screenshots of the games in a tiled fashion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# render the envs\n",
    "import time \n",
    "\n",
    "# list of envs \n",
    "num_envs = 3\n",
    "envs = [lambda: gym.make(\"BreakoutNoFrameskip-v4\") for i in range(num_envs)]\n",
    "\n",
    "# Vec Env \n",
    "envs = SubprocVecEnv(envs)\n",
    "\n",
    "init_obs = envs.reset()\n",
    "\n",
    "for i in range(1000):\n",
    "    actions = [envs.action_space.sample() for i in range(num_envs)]\n",
    "    envs.step(actions)\n",
    "    envs.render()\n",
    "    time.sleep(0.001)\n",
    "\n",
    "envs.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find more about vectorized environments [here](https://stable-baselines.readthedocs.io/en/master/guide/vec_envs.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
